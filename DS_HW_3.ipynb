{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEmfUwL9hKbtQaazUkCEW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kagev/DataScience/blob/main/DS_HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання**\n",
        "\n",
        "\n",
        "Дане домашнє завдання буде повністю пов'язане з лінійною регресією та її реалізацією. Отож розіб'ємо наше домашнє завдання на декілька частин:"
      ],
      "metadata": {
        "id": "BgrHE39ekkB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "напишіть функцію гіпотези лінійної регресії у векторному вигляді;"
      ],
      "metadata": {
        "id": "uYFrL5jBkrzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6eVWb-X6kejp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def hypothesis(X, w):\n",
        "    return np.dot(X, w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Створіть функцію для обчислення функції втрат у векторному вигляді"
      ],
      "metadata": {
        "id": "jgkw29brlCEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(X, y, w):\n",
        "    m = len(y)\n",
        "    predictions = hypothesis(X, w)\n",
        "    return (1/(2*m)) * np.sum((predictions - y) ** 2)\n"
      ],
      "metadata": {
        "id": "zs9MWjEYlERu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реалізуйте один крок градієнтного спуску"
      ],
      "metadata": {
        "id": "LkpMFK80lGM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(X, y, w, learning_rate):\n",
        "    m = len(y)\n",
        "    predictions = hypothesis(X, w)\n",
        "    gradient = np.dot(X.T, (predictions - y)) / m\n",
        "    w -= learning_rate * gradient\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "_VQHKRyJlKDC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Знайдіть найкращі параметри за допомогою написаних вами функцій"
      ],
      "metadata": {
        "id": "y6RMyxDMlMM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, learning_rate, iterations):\n",
        "    loss_history = []\n",
        "    for i in range(iterations):\n",
        "        w = gradient_descent_step(X, y, w, learning_rate)\n",
        "        loss_history.append(loss_function(X, y, w))\n",
        "    return w, loss_history\n",
        "\n",
        "# Приклад ініціалізації та виклику\n",
        "X = np.array([[1, 2100, 3], [1, 1600, 2], [1, 2400, 4]])  # Не забудьте додати стовпчик з одиницями для зміщення\n",
        "y = np.array([400000, 330000, 369000])\n",
        "w = np.zeros(X.shape[1])  # Ініціалізуємо ваги нулями\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "best_w, loss_hist = gradient_descent(X, y, w, learning_rate, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd7h_dQqlOZB",
        "outputId": "0cf4ad6b-086e-4f73-8e54-6545150b1858"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d5f24c7cce39>:4: RuntimeWarning: overflow encountered in square\n",
            "  return (1/(2*m)) * np.sum((predictions - y) ** 2)\n",
            "<ipython-input-3-20e314a9e848>:5: RuntimeWarning: invalid value encountered in subtract\n",
            "  w -= learning_rate * gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Знайдіть параметри за допомогою аналітичного рішення"
      ],
      "metadata": {
        "id": "vSnkNDvGlPnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_solution(X, y):\n",
        "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "w_analytical = analytical_solution(X, y)\n"
      ],
      "metadata": {
        "id": "q0UvwbK1lR5n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Порівняйте з LinearRegression з бібліотеки Scikit-learn"
      ],
      "metadata": {
        "id": "xronHuPelU8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "w_sklearn = np.hstack([model.intercept_, model.coef_[1:]])\n"
      ],
      "metadata": {
        "id": "cVluUN6MlVqo"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}